#!/usr/bin/python
# bump alpha-5
import sys; sys.path.insert(0, 'pylib')
import time, os, threading, traceback, hashlib, math, random, socket
from datetime import datetime, timedelta
from collections import namedtuple, defaultdict
from itertools import count
from six import integer_types
from hosted import config, node, device, monotonic_time
from hosted.scheduler import verified_timezone, utc_to_local
from hosted.p2p import OrderedEventGroup

config.restart_on_update()


device.gpio.set_pin_value(26,  high=True)
device.gpio.set_pin_value(19,  high=True)
print("on")

lua = node.rpc()

PRELOAD = 1.5
SERIAL = os.environ['SERIAL']

def log(msg, name='controller'):
    print >>sys.stderr, "[{}] {}".format(name, msg)

def sleep_until(t):
    now = monotonic_time()
    delta = t - now
    if delta > 0:
        time.sleep(delta)

class OnDemandPop(object):
    def __init__(self):
        self._pop = None

    def submit(self, pop_event):
        if not self._pop:
            try:
                self._pop = device.pop()
            except Exception as err:
                log("cannot initialize proof of play: %s" % (err,))
                return

        log("pop event: %r" % (pop_event,))
        self._pop.log(
            pop_event['play_start'],
            pop_event['duration'],
            pop_event['asset_id'],
            pop_event['asset_filename'],
        )
OnDemandPop = OnDemandPop()

@lua.call
def submit_pop(pop_event):
    log("pop event: %r" % (pop_event,))
    OnDemandPop.submit(pop_event)

DisplayStatus = namedtuple("DisplayStatus", "serial config_state peer_info")

def forward_time(shared_time):
    lua.time(shared_time, time.time())

class TVPower(object):
    def __init__(self):
        log('Turning screen on')
        device.turn_screen_on()
        self._is_on = True
    def set(self, be_on):
        if be_on ^ self._is_on:
            log('Turning screen to be on: %s' % (be_on,))
            device.screen(be_on)
            self._is_on = be_on
    def on(self):
        self.set(True)
    def off(self):
        self.set(False)
TVPower = TVPower()

class WallGroup(OrderedEventGroup):
    def __init__(self):
        super(WallGroup, self).__init__()
        self._config_state = None
        self._playlist = None

        self._all_synced = False
        self._status_by_device_id = {}

        self._peer_msg_handlers = {
            'status_update': self.handle_status_update,
        }
        self._leader_msg_handlers = {
            'preload': lua.preload,
            'switch': lua.switch,
            'time': forward_time,
            'tv-on': TVPower.on,
            'tv-off': TVPower.off,
        }

        thread = threading.Thread(target=self.leader_msg_event_loop)
        thread.daemon = True
        thread.start()

        thread = threading.Thread(target=self.status_loop)
        thread.daemon = True
        thread.start()


    def add_msg_handler(self, msg_type, handler):
        self._peer_msg_handlers[msg_type] = handler

    def on_peer_removed(self, peer_info):
        super(WallGroup, self).on_peer_removed(peer_info)
        if peer_info.device_id in self._status_by_device_id:
            del self._status_by_device_id[peer_info.device_id]

    def handle_status_update(self, peer_info, msg):
        status = DisplayStatus(peer_info=peer_info, **msg)
        self._status_by_device_id[peer_info.device_id] = status

    def on_peer_message(self, msg, peer_info):
        super(WallGroup, self).on_peer_message(msg, peer_info)
        msg_type, values = msg['msg_type'], msg['values']
        handler = self._peer_msg_handlers.get(msg_type)
        if handler:
            handler(peer_info, values)

    def msg_to_leader(self, msg_type, values):
        self.send_to_leader(msg_type=msg_type, values=values)

    def send_leader_status_update(self, **values):
        self.msg_to_leader('status_update', values)

    def status_update(self):
        playlist, metadata = config.playlist, config.metadata

        config_hash = hashlib.md5(str(metadata['config_rev']))
        for item in playlist:
            config_hash.update(item['asset']['asset_name'])
        config_state = config_hash.hexdigest()[:16]

        self.send_leader_status_update(
            serial = SERIAL,
            config_state = config_state,
        )

        node['/debug/update'](dict(
            peer = dict(
                serial = SERIAL,
                config_state = config_state,
                is_leader = self.is_leader,
            ),
            peers = [peer._asdict() for peer in self.peers],
            leader = self.leader.peer_info._asdict() if self.leader else False,
        ))

        # Unless we're the leader, everything beyond this point is irrelevant
        if self.is_leader:
            # Check if all devices have the same configuration
            config_states = set(
                status.config_state
                for status in self._status_by_device_id.itervalues()
            )
            self._all_synced = len(config_states) == 1
            if self._all_synced:
                self._config_state = config_states.pop()
                self._playlist = playlist

            node['/debug/update'](dict(
                controller = dict(
                    all_synced = self._all_synced,
                    common_state = self._config_state,
                )
            ))
        else:
            node['/debug/update'](dict(
                controller = {})
            )


    def dashboard_update(self):
        try:
            device.kv.update(dict(
                leader_id = self.leader.peer_info.device_id if self.leader else None,
                is_leader = ('0', '1')[self.is_leader],
                all_synced = ('0', '1')[self._all_synced],
                peers = len(self.peers),
            ))
        except device.kv.Error as err:
            log("cannot update dashboard: %s" % (err,))

    # Running on all peers
    def status_loop(self):
        for i in count():
            try:
                self.status_update()
                if i % 5 == 0:
                    self.dashboard_update()
            except:
                traceback.print_exc()
            time.sleep(2)

    # Running on all peers
    def leader_msg_event_loop(self):
        for delay, event in self.events():
            log("event %r delivery time offset is %f" % (event['fn'], delay,))
            handler = self._leader_msg_handlers.get(event['fn'])
            if handler:
                handler(*event['args'])

    def synced_call(self, offset, fn, *args):
        self.send_event(self.time() + offset, dict(
            fn = fn,
            args = args,
        ))

    def common_playlist(self):
        return self._config_state, self._playlist

Item = namedtuple("Item", "config_state item_idx duration cnt rnd")

class AlternativeLooper(object):
    def __init__(self):
        self._count = count(0).next
    def next(self):
        return self._count() % (1*2*3*4*5*6*7*8*9*10)

class ItemGenerator(object):
    def __init__(self, wall):
        self._wall = wall
        self._item_idx = 0
        self._item_idx_count = defaultdict(AlternativeLooper)

    def get_timezone(self):
        return verified_timezone(config.metadata['timezone'])

    def get_next(self):
        config_state, playlist = self._wall.common_playlist()
        if not playlist:
            return None

        tz = self.get_timezone()
        now = utc_to_local(datetime.utcnow(), tz).replace(
            second = 0, microsecond = 0,
        )

        node['/debug/update'](dict(
            time = dict(
                local = str(now),
                tz = tz.zone,
            )
        ))

        for probe in xrange(len(playlist)):
            self._item_idx = (self._item_idx + 1) % len(playlist)
            item = playlist[self._item_idx]
            duration = item['duration']
            if duration == 0:
                duration = 10
                metadata = item['asset']['metadata']
                if 'duration' in metadata:
                    duration = metadata['duration']
            duration = max(2, duration)
            end = now + timedelta(minutes=max(1, int(math.ceil(duration/60.))))
            spans = item['schedule'].spans_between(tz, now, end)
            if not spans:
                # Not scheduled within now and the maximum potential end date? skip
                continue
            return Item(
                config_state = config_state,
                item_idx = self._item_idx+1,
                duration = duration,
                cnt = self._item_idx_count[self._item_idx].next(),
                rnd = random.randint(0, 2**20),
            )
        return None

    def has_urgent_item(self):
        return False

class RemoteControlItemGenerator(object):
    def __init__(self, wall):
        self._sock = None
        self._port = None
        self._wall = wall

        self._last_msg_ids = []
        self._wall.add_msg_handler('trigger', self.handle_leader_trigger)

        self._item_id_queue = []
        self._urgent = False

        self._item_idx_count = defaultdict(AlternativeLooper)

        thread = threading.Thread(target=self.listen_thread)
        thread.daemon = True
        thread.start()

    def update_socket_state(self):
        if self._port and config.remote_port != self._port:
            log("closing UDP socket")
            self._sock.close()
            self._sock = None
            self._port = 0
        if not self._port and config.remote_port:
            log("opening UDP port %d" % (config.remote_port,))
            self._sock = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)
            self._sock.bind(('0.0.0.0', config.remote_port))
            self._sock.settimeout(1)
            self._port = config.remote_port

    def listen_thread(self):
        while 1:
            try:
                self.update_socket_state()
                if self._sock:
                    msg, addr = self._sock.recvfrom(1024)
                    self.handle_udp_msg(msg)
                else:
                    time.sleep(1)
            except socket.timeout:
                pass
            except:
                traceback.print_exc()

    def handle_udp_msg(self, msg):
        event, msg_id, data = msg.split(':', 2)
        if event != 'trigger':
            return
        if msg_id in self._last_msg_ids:
            return
        self._last_msg_ids.append(msg_id)
        while len(self._last_msg_ids) > 4:
            self._last_msg_ids.pop(0)
        self.parse_trigger(data)

    def parse_trigger(self, trigger_cmd):
        def parse_id(v):
            try:
                return int(v)
            except:
                return v.decode('utf8')
        item_ids = [parse_id(v) for v in trigger_cmd.split(',')]
        log("TRIGGER EVENT ->> %r" % (item_ids,))
        self.send_trigger_to_leader(item_ids)

    def send_trigger_to_leader(self, item_ids):
        self._wall.msg_to_leader('trigger', item_ids)

    def handle_leader_trigger(self, peer_info, item_ids):
        log('-->> TRIGGER EVENT: %r %r' % (peer_info, item_ids))
        self._item_id_queue = item_ids
        self._urgent = True

    def get_next(self):
        self._urgent = False
        config_state, playlist = self._wall.common_playlist()
        if not playlist:
            return None

        while self._item_id_queue:
            item_id = self._item_id_queue.pop(0)
            if isinstance(item_id, integer_types):
                if not 0 <= item_id < len(playlist):
                    continue
                item, item_idx = playlist[item_id], item_id
            else:
                for item_idx, item in enumerate(playlist):
                    # search file filename
                    if item_id == 'f:%s' % item['asset']['filename']:
                        break
                    # search by asset_id: either numeric for backend
                    # assets or string for package assets
                    if item_id == 'a:%s' % item['asset']['asset_id']:
                        break
                else:
                    continue
            duration = max(2, item['duration'])
            return Item(
                config_state = config_state,
                item_idx = item_idx+1,
                duration = duration,
                cnt = self._item_idx_count[item_idx].next(),
                rnd = random.randint(0, 2**20),
            )
        return None

    def has_urgent_item(self):
        return self._urgent

class CombinedItemFeed(object):
    def __init__(self, *feeds):
        self._feeds = feeds

    def sleep_and_get_next(self, max_wait_until):
        now = monotonic_time()
        is_urgent = False
        while now < max_wait_until:
            if self.has_urgent_item():
                is_urgent = True
                break
            sleep = min(0.25, max_wait_until - now)
            sleep_until(now + sleep)
            now = monotonic_time()

        for feed_idx, feed in enumerate(self._feeds):
            item = feed.get_next()
            if item:
                log("using item from feed %d" % (feed_idx,))
                return item, is_urgent
        return None, is_urgent

    def has_urgent_item(self):
        for feed in self._feeds:
            if feed.has_urgent_item():
                return True
        return False

def fallback_item():
    return Item( # will trigger fallback on lua display
        config_state = '',
        item_idx = -1,
        duration = 5,
        cnt = 0,
        rnd = 0,
    )

def controller(wall, item_feed):
    MAX_SUSPEND_DEPTH = 4

    next_switch = monotonic_time() + PRELOAD + 0.1
    suspend_depth = 0

    while 1:
        # Wait at most until PRELOAD seconds before the
        # current item ends/next item starts
        item, is_urgent = item_feed.sleep_and_get_next(next_switch - PRELOAD)

        if item is None:
            item = fallback_item()
            if config.blank:
                # Every time no item is scheduled, suspend_depth
                # gets increased if setup is configured to suspend
                # displays. Once MAX_SUSPEND_DEPTH is reached, the
                # screen is turned off.
                suspend_depth = min(MAX_SUSPEND_DEPTH, suspend_depth+1)
        elif suspend_depth > 0:
            # If an item could be scheduled, but suspend_depth
            # hasn't returned to zero yet, play fallback item as
            # a placeholder while the display is slowly turning
            # back on.
            item = fallback_item()
            suspend_depth -= 1

        if is_urgent:
            next_switch = monotonic_time() + PRELOAD/2

        wall.synced_call(0.5, 'time', monotonic_time()+0.5)

        if suspend_depth == MAX_SUSPEND_DEPTH and config.blank:
            wall.synced_call(0.5, 'tv-off')
        else:
            wall.synced_call(0.5, 'tv-on')

        # Send preloading instruction to peers.
        wall.synced_call(0.2, 'preload', item._asdict())

        # Now figure out how much time we have left until
        # the switch. Schedule switching accordingly.
        switch_time = next_switch - monotonic_time()
        wall.synced_call(switch_time, 'switch')

        # Wait until all screens have switched
        sleep_until(next_switch)

        # content has switched now. Decide when to switch next.
        next_switch = next_switch + item.duration

wall = WallGroup()

remote_control_item_generator = RemoteControlItemGenerator(wall)

@lua.call
def trigger(trigger_cmd):
    remote_control_item_generator.parse_trigger(trigger_cmd)

item_feed = CombinedItemFeed(
    remote_control_item_generator,
    ItemGenerator(wall),
)

controller(wall, item_feed)
